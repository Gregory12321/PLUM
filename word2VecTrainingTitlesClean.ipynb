{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import Phrases\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.special import expit as sigmoid \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ander\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1 Fail\n",
    "# 2 Success\n",
    "# 8 Commons\n",
    "# 9 Lords\n",
    "#18 failed commons\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    " \n",
    "\n",
    "def cleanFilesInFolder(startSession, endSession, house_and_status=[18, 19, 28, 29]):\n",
    "    for code in house_and_status:\n",
    "        file_name = f\"CSVFilesMLP/{code}_{startSession} to {endSession}.csv\"\n",
    "        try:\n",
    "            df = pd.read_csv(file_name)\n",
    "            long_titles = df['Long Title'].astype(str)  \n",
    "            output_file_name = f\"cleanedText/{code}_{startSession} to {endSession}.txt\"\n",
    "            \n",
    "            with open(output_file_name, \"w\", encoding=\"utf-8\") as file:\n",
    "                for title in long_titles:\n",
    "                    modified_title = title.replace('A Bill to', '').replace('; and for connected purposes', '') \n",
    "                    modified_title = modified_title.lower()    \n",
    "                    file.write(modified_title)\n",
    "        except UnicodeEncodeError as e:\n",
    "            print(f\"Encoding error in file {file_name}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing {file_name}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "def splitSentences(text):\n",
    "    \n",
    "    sentences = re.split(r'\\.\\s+', text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "    filtered_sentences = []\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "        filtered_sentences.append(filtered_words)\n",
    "        \n",
    "    return filtered_sentences\n",
    "\n",
    "def splitDocument(startSession, endSession, house_and_status = [18,19,28,29]):\n",
    "    for code in house_and_status: \n",
    "        file_name = f\"cleanedText/{code}_{startSession} to {endSession}.txt\"\n",
    "        try:\n",
    "            with open(file_name, 'r', encoding=\"utf-8\") as file:\n",
    "                for line in file:\n",
    "                    yield splitSentences(line)\n",
    "            print(f\"Read File{file_name}\")\n",
    "        except UnicodeDecodeError as e:\n",
    "            print(f\"Unicode decode error\")\n",
    "# returns a list of lists . List each sentnece, and inside there is a list of words for each sentence\n",
    "\n",
    "def trainW2V(key, T=50):\n",
    "    sentences = categorizedBillSentences[key]\n",
    "    for epoch in range(T):\n",
    "        print(f\"{epoch}\", end=\"\")\n",
    "        np.random.shuffle(sentences)\n",
    "        models[key].train(sentences, total_examples=len(sentences), epochs=1)\n",
    "        models[key].alpha *= 0.9\n",
    "        models[key].min_alpha = models[key].alpha\n",
    "    print(\".\")\n",
    "    \n",
    "\n",
    "\n",
    "def nearby(word, g):\n",
    "    print(word)\n",
    "    print(f\"{g}:\", end=\" \")\n",
    "    try:\n",
    "        if word in models[g].wv.key_to_index:  \n",
    "            for (w, v) in models[g].wv.most_similar([word]):\n",
    "                print(w, end=\" \")\n",
    "        else:\n",
    "            print(\"Word not in vocabulary!\", end=\" \")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# input (sentence: list of words, model: gensim model, window: window= windowSize of word2vec, \n",
    "#debug: print intermediate calculations for debugging)\n",
    "\n",
    "def score_sentence(sentence, model, window=5, debug=False):\n",
    "    log_prob = 0.0 # total log prob for the sentence\n",
    "    sentence_length = len(sentence)\n",
    "    word_pair_probs = []  \n",
    "\n",
    "    # Code for equation 1 \n",
    "    for index, center_word in enumerate(sentence):\n",
    "        if center_word not in model.wv:\n",
    "            if debug:\n",
    "                print(f\"Center word '{center_word}' not in vocabulary.\")\n",
    "            continue\n",
    "        center_vector = model.wv[center_word]\n",
    "\n",
    "        start = max(0, index - window)\n",
    "        end = min(sentence_length, index + window + 1)\n",
    "\n",
    "        for j in range(start, end):\n",
    "            if j == index:\n",
    "                continue\n",
    "            context_word = sentence[j]\n",
    "            if context_word not in model.wv:\n",
    "                if debug:\n",
    "                    print(f\"Context word '{context_word}' not in vocabulary.\")\n",
    "                continue\n",
    "            context_vector = model.wv[context_word]\n",
    "\n",
    "            dot_product = np.dot(center_vector, context_vector)\n",
    "            prob = sigmoid(dot_product)\n",
    "\n",
    "            word_pair_probs.append((center_word, context_word, prob))\n",
    "\n",
    "            log_prob += np.log(prob + 1e-10)\n",
    "\n",
    "    if debug:\n",
    "        print(\"\\n--- Word Pair Probabilities ---\")\n",
    "        for center, context, prob in word_pair_probs:\n",
    "            print(f\"p({context} | {center}) = {prob:.6f}\")\n",
    "\n",
    "    return log_prob\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Score an entire document (S sentences) under all models (Equation 2)\n",
    "# input (sentencces:  a list of sentences ,models: the dictionary of models, window: the window size for score sentences)\n",
    "# outpur: a sentences x categories (failed , succesful ....) with eahc sentence score according to score_sentence\n",
    "\n",
    "def score_document(sentences, models, window=5):\n",
    "    \"\"\"\n",
    "    Compute the score x category matrix of sentence scores for a document.\n",
    "    \n",
    "    sentences: list of sentences, each sentence is a list of words\n",
    "    models: dict of {category: Word2Vec model}\n",
    "    \"\"\"\n",
    "    S = len(sentences)\n",
    "    C = len(models)\n",
    "    \n",
    "    sentence_scores = np.zeros((S, C))\n",
    "    \n",
    "    for s_idx, sentence in enumerate(sentences):\n",
    "        for c_idx, (category, model) in enumerate(models.items()):\n",
    "            sentence_scores[s_idx, c_idx] = score_sentence(sentence, model, window)\n",
    "    \n",
    "    return sentence_scores\n",
    "\n",
    "\n",
    "\n",
    "# calculate document probabilities (Equation 5)\n",
    "\n",
    "# input: the sxc array\n",
    "# output: a 1x cateories array with the average score for all sentences in document \n",
    "def document_probabilities(sentence_scores):\n",
    "\n",
    "    return sentence_scores.mean(axis=0)\n",
    "\n",
    "\n",
    "\n",
    "# compute class probabilities ( Equation 3)\n",
    "\n",
    "# input:  the array from document_probabilities\n",
    "#ouput: normalized probabilities after bayes rule is applied #todo: change the priors to correspond to each class \n",
    "def class_probabilities(doc_probs):\n",
    "    \"\"\"\n",
    "    Compute class probabilities using Bayes rule.\n",
    "    Assuming uniform priors.\n",
    "    \"\"\"\n",
    "    priors = np.ones(len(doc_probs)) / len(doc_probs)\n",
    "    # bayes rule\n",
    "    probs = (doc_probs * priors) / np.sum(doc_probs * priors)\n",
    "    return probs\n",
    "\n",
    "\n",
    "\n",
    "# classify the document (Equation 6)\n",
    "# checks which of the numbers in the 1d array from document probabilities (the average across the classes ) is biggest and returns the index and array (for debuging) \n",
    " \n",
    "def classify_document(sentence_scores):\n",
    "    doc_probs = document_probabilities(sentence_scores)\n",
    "    predicted_class_idx = np.argmax(doc_probs)\n",
    "    return predicted_class_idx, doc_probs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read FilecleanedText/18_17 to 39.txt\n",
      "Read FilecleanedText/19_17 to 39.txt\n",
      "Read FilecleanedText/28_17 to 39.txt\n",
      "Read FilecleanedText/29_17 to 39.txt\n",
      "Sentences retreived\n",
      "2830\n",
      "Read FilecleanedText/18_17 to 39.txt\n",
      "Read FilecleanedText/19_17 to 39.txt\n",
      "Read FilecleanedText/28_17 to 39.txt\n",
      "Read FilecleanedText/29_17 to 39.txt\n",
      "Sentences per category\n",
      "Sentences in FailedCommons 2019.0\n",
      "Sentences in FailedCommons 282.0\n",
      "Sentences in FailedCommons 448.0\n",
      "Sentences in FailedCommons 81.0\n",
      "FailedCommons:012345678910111213141516171819202122232425262728293031323334353637383940414243444546474849.\n",
      "FailedLords:012345678910111213141516171819202122232425262728293031323334353637383940414243444546474849.\n",
      "SuccesCommons:012345678910111213141516171819202122232425262728293031323334353637383940414243444546474849.\n",
      "SuccessLords:012345678910111213141516171819202122232425262728293031323334353637383940414243444546474849.\n",
      "dict_keys(['FailedCommons', 'FailedLords', 'SuccesCommons', 'SuccessLords'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Go from CSV files to 4 trained word2Vec models , 1 per category\n",
    "\n",
    "startSession = 17\n",
    "endSession= 39\n",
    "\n",
    "# Convert from csv file to text file with just the long titles of the bills for all 4 files of te start-end session range\n",
    "cleanFilesInFolder(startSession, endSession)\n",
    "\n",
    "#combine sentences from all 4 files into 1 list of lists. Each sentence a list of words \n",
    "allSentences = [sentence for listOfSentneces in splitDocument(startSession, endSession) for sentence in listOfSentneces]\n",
    "\n",
    "print(\"Sentences retreived\")\n",
    "print(len(allSentences))\n",
    "\n",
    "\n",
    "# the catgeories are also used in scoring, the index for categories relates to this  \n",
    "houseDictionary = {'FailedCommons': [18], 'FailedLords': [19], \"SuccesCommons\": [28], \"SuccessLords\":[29]}\n",
    "[g for g in houseDictionary]\n",
    "\n",
    "\n",
    "\n",
    "# populate a dictionary: key: category of bill, value: list of lists with sentences from bills of that category(FailedCommons, etc)\n",
    "categorizedBillSentences = {key: [sentence for listOfSentneces in splitDocument(startSession, endSession, houseDictionary[key]) for sentence in listOfSentneces] for key in houseDictionary}\n",
    "\n",
    "# creates series with how many sentnces for each catgory(debugging)\n",
    "numberBills = pd.Series({key: len(categorizedBillSentences[key]) for key in houseDictionary}, dtype=\"float64\" )\n",
    "\n",
    "print(\"Sentences per category\")\n",
    "print(f\"Sentences in FailedCommons {numberBills[0]}\")\n",
    "print(f\"Sentences in FailedCommons {numberBills[1]}\")\n",
    "print(f\"Sentences in FailedCommons {numberBills[2]}\")\n",
    "print(f\"Sentences in FailedCommons {numberBills[3]}\")\n",
    "\n",
    "\n",
    "# populate houseDictionary with bills of appropaite category as lists of lists\n",
    "for key in houseDictionary:\n",
    "    for i in range(len(categorizedBillSentences[key])):\n",
    "        categorizedBillSentences[key][i] = [word for word in categorizedBillSentences[key][i]]\n",
    "\n",
    "\n",
    "#creates a dictionary of word2vec models initialized with the vocabulary of all sentences         \n",
    "\n",
    "\n",
    "models = { }\n",
    "\n",
    "\n",
    "for key in houseDictionary:\n",
    "    models[key] = Word2Vec(allSentences, workers=4, hs=1, negative=0)\n",
    "    models[key].build_vocab(allSentences) \n",
    "    \n",
    "    \n",
    "\n",
    "# trains each of the initialized models only with text from each specific category. 4 models trained with different bill text\n",
    "for key in houseDictionary:\n",
    "    print(key, end=\":\")\n",
    "    trainW2V(key)\n",
    "\n",
    "\n",
    "print(models.keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence 1:\n",
      "Predicted class: SuccesCommons\n",
      "Document probabilities: [-82.41483574 -80.63811194 -71.21155376 -78.84841476]\n",
      "Class probabilities: [0.26321123 0.25753684 0.2274309  0.25182102]\n",
      "\n",
      "Sentence 2:\n",
      "Predicted class: SuccesCommons\n",
      "Document probabilities: [-98.81467435 -62.1529499  -12.55124981 -64.851167  ]\n",
      "Class probabilities: [0.41454318 0.26074145 0.05265448 0.2720609 ]\n",
      "\n",
      "Sentence 3:\n",
      "Predicted class: FailedLords\n",
      "Document probabilities: [-165.92863017   -2.96225362  -90.3188394   -83.54459483]\n",
      "Class probabilities: [0.48410369 0.0086425  0.26350898 0.24374484]\n",
      "\n",
      "Sentence 4:\n",
      "Predicted class: SuccessLords\n",
      "Document probabilities: [-38.38523628 -32.39724256 -38.09664073  -0.27078963]\n",
      "Class probabilities: [0.35167447 0.2968142  0.34903044 0.0024809 ]\n"
     ]
    }
   ],
   "source": [
    "#Score sentences from the next three sessions(compared to the sessions the models were trained with) and evaluate how accurate the models are \n",
    "\n",
    "sentences = [\n",
    "    \"A Bill to require the Secretary of State to promote and secure youth services and provision of a requisite standard; to impose a duty on local authorities to provide youth services and establish local youth service partnerships with youth participation; and for connected purposes.\",\n",
    "    \n",
    "    \"A Bill to Authorise the use of resources for the year ending with 31 March 2020; to authorise both the issue of sums out of the Consolidated Fund and the application of income for that year; and to appropriate the supply authorised for that year by this Act and by the Supply and Appropriation (Anticipation and Adjustments) Act 2019.\",\n",
    "    \n",
    "    \"A Bill to make provision for unaccompanied asylum seeking children to receive legal advice and for extending the deadline for an unaccompanied asylum seeking child to appeal an asylum decision\",\n",
    "    \n",
    "    \"To confer powers upon New Southgate Cemetery and Crematorium Limited and the National Spiritual Assembly of the BahÃ¡'is of the United Kingdom to extinguish rights of burial and disturb human remains in respect of New Southgate Cemetery for the purpose of increasing the space for interments; and for connected purposes.\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "categories = list(models.keys())\n",
    "\n",
    "for i, sentence_text in enumerate(sentences, 1):\n",
    "    document = splitSentences(sentence_text)  \n",
    "    sentence_scores = score_document(document, models, window=5)\n",
    "    doc_probs = document_probabilities(sentence_scores)\n",
    "    probs = class_probabilities(doc_probs)\n",
    "    predicted_idx, doc_probs = classify_document(sentence_scores)\n",
    "\n",
    "    print(f\"\\nSentence {i}:\")\n",
    "    print(f\"Predicted class: {categories[predicted_idx]}\")\n",
    "    print(f\"Document probabilities: {doc_probs}\")\n",
    "    print(f\"Class probabilities: {probs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read FilecleanedText/18_35 to 37.txt\n",
      "Read FilecleanedText/19_35 to 37.txt\n",
      "Read FilecleanedText/28_35 to 37.txt\n",
      "Read FilecleanedText/29_35 to 37.txt\n",
      " Number of sentences for FailedCommons is 617\n",
      "The accuracy for FailedCommons is \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ander\\miniconda3\\envs\\pdiot\\lib\\site-packages\\ipykernel_launcher.py:77: RuntimeWarning: Mean of empty slice.\n",
      "C:\\Users\\ander\\miniconda3\\envs\\pdiot\\lib\\site-packages\\numpy\\core\\_methods.py:182: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "C:\\Users\\ander\\miniconda3\\envs\\pdiot\\lib\\site-packages\\ipykernel_launcher.py:92: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Prediction: 0.5186385737439222\n",
      " Number of sentences for FailedLords is 28\n",
      "The accuracy for FailedLords is \n",
      "Correct Prediction: 0.8928571428571429\n",
      " Number of sentences for SuccesCommons is 201\n",
      "The accuracy for SuccesCommons is \n",
      "Correct Prediction: 0.5124378109452736\n",
      " Number of sentences for SuccessLords is 8\n",
      "The accuracy for SuccessLords is \n",
      "Correct Prediction: 0.75\n",
      "Model for FailedCommons saved as FailedCommons_word2vec_titles.model\n",
      "Model for FailedLords saved as FailedLords_word2vec_titles.model\n",
      "Model for SuccesCommons saved as SuccesCommons_word2vec_titles.model\n",
      "Model for SuccessLords saved as SuccessLords_word2vec_titles.model\n"
     ]
    }
   ],
   "source": [
    "#Prepare data to test models. Split csv files into list of long titles per category\n",
    "\n",
    "\n",
    "#Split a file into complete sentences. Files for success commons 28-30\n",
    "\n",
    "def scoreSentences(listOfSentences, modelsList, predictedCategory):\n",
    "    success= 0\n",
    "    fail =0\n",
    "    \n",
    "    categories = list(modelsList.keys())\n",
    "    \n",
    "    for i, sentence_text in enumerate(listOfSentences, 1):\n",
    "        document = splitSentences(sentence_text)\n",
    "        sentence_scores = score_document(document, modelsList, window=5)\n",
    "        doc_probs = document_probabilities(sentence_scores)\n",
    "        probs = class_probabilities(doc_probs)\n",
    "        predicted_idx, doc_probs = classify_document(sentence_scores)\n",
    "        \n",
    "       # print(f\" Predicting Sentence {document}\")\n",
    "        #print(f\"\\nSentence {i}:\")\n",
    "        #print(f\"Predicted class: {categories[predicted_idx]}\")\n",
    "       # print(f\"Document probabilities: {doc_probs}\")\n",
    "        #print(f\"Class probabilities: {probs}\")\n",
    "        \n",
    "        if (categories[predicted_idx] == predictedCategory):\n",
    "            success = success + 1\n",
    "        else:\n",
    "            fail = fail + 1\n",
    "    total = fail + success\n",
    "    if total > 0:\n",
    "        accuracy = success / total\n",
    "        print(f\"Correct Prediction: {accuracy}\")\n",
    "    else:\n",
    "        print(\"Error in predicition.\")\n",
    "\n",
    "\n",
    "startSessionTest = 35\n",
    "endSessionTest = 37\n",
    "\n",
    "\n",
    "def splitSentencesForTest(text):\n",
    "    \n",
    "    sentences = re.split(r'\\.\\s+', text)    \n",
    "    return sentences\n",
    "\n",
    "\n",
    "def splitDocumentTest(startSession, endSession, house_and_status):\n",
    "    for code in house_and_status: \n",
    "        file_name = f\"cleanedText/{code}_{startSession} to {endSession}.txt\"\n",
    "        try:\n",
    "            with open(file_name, 'r', encoding=\"utf-8\") as file:\n",
    "                for line in file:\n",
    "                    sentences = splitSentencesForTest(line.strip())\n",
    "                    for sentence in sentences:\n",
    "                        yield sentence \n",
    "            print(f\"Read File{file_name}\")\n",
    "        except UnicodeDecodeError as e:\n",
    "            print(f\"Unicode decode error\")\n",
    "\n",
    "\n",
    "\n",
    "codes= [18,19,28,19]\n",
    "\n",
    "\n",
    "testDictionary = {'FailedCommons': [18], 'FailedLords': [19], \"SuccesCommons\": [28], \"SuccessLords\":[29]}\n",
    "\n",
    "\n",
    "for key, codes in testDictionary.items():\n",
    "    testDictionary[key] = [sentence for sentence in splitDocumentTest(startSessionTest, endSessionTest, house_and_status=codes)]\n",
    "\n",
    "#for key, sentences in testDictionary.items():\n",
    " #   print(f\"{key}: {sentences[:5]}\")\n",
    "    \n",
    "      \n",
    "\n",
    "\n",
    "for key, code in testDictionary.items():\n",
    "    testSentences = testDictionary[key]\n",
    "    \n",
    "    print (f\" Number of sentences for {key} is {len(testSentences)}\")\n",
    "    print(f\"The accuracy for {key} is \") \n",
    "    scoreSentences(testSentences, models, key)\n",
    "    \n",
    "\n",
    "# save the models \n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "for key, model in models.items():\n",
    "    filename = f\"{key}_word2vec_titles.model\"\n",
    "    model.save(filename)\n",
    "    print(f\"Model for {key} saved as {filename}\")\n",
    "\n",
    "    \n",
    "# dict_keys(['FailedCommons', 'FailedLords', 'SuccesCommons', 'SuccessLords'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
