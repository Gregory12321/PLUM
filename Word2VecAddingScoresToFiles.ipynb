{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import Phrases\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.special import expit as sigmoid \n",
    "import os\n",
    "import re\n",
    "import fitz  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ander\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Function Definitions \n",
    "\n",
    "# 1 Fail\n",
    "# 2 Success\n",
    "# 8 Commons\n",
    "# 9 Lords\n",
    "#18 failed commons\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def splitSentences(text):\n",
    "    \n",
    "    sentences = re.split(r'\\.\\s+', text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "    filtered_sentences = []\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "        filtered_sentences.append(filtered_words)\n",
    "        \n",
    "    return filtered_sentences\n",
    "\n",
    "def splitDocument(house_and_status = [18,19,28,29]):\n",
    "    for code in house_and_status: \n",
    "        file_name = f\"cleanedTextFull/{code}/Training/training_text{code}.txt\"\n",
    "        try:\n",
    "            with open(file_name, 'r', encoding=\"utf-8\") as file:\n",
    "                for line in file:\n",
    "                    yield splitSentences(line)\n",
    "            print(f\"Read File{file_name}\")\n",
    "        except UnicodeDecodeError as e:\n",
    "            print(f\"Unicode decode error\")\n",
    "# returns a list of lists . List each sentnece, and inside there is a list of words for each sentence\n",
    "\n",
    "\n",
    "def splitDocumentALL(startSession, endSession, house_and_status = [18,19,28,29]):\n",
    "    for code in house_and_status: \n",
    "        file_name = f\"cleanedTextFull/{code}_{startSession} to {endSession} fullText.txt\"\n",
    "        try:\n",
    "            with open(file_name, 'r', encoding=\"utf-8\") as file:\n",
    "                for line in file:\n",
    "                    yield splitSentences(line)\n",
    "            print(f\"Read File{file_name}\")\n",
    "        except UnicodeDecodeError as e:\n",
    "            print(f\"Unicode decode error\")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# input (sentence: list of words, model: gensim model, window: window= windowSize of word2vec, \n",
    "#debug: print intermediate calculations for debugging)\n",
    "\n",
    "def score_sentence(sentence, model, window=7, debug=False):\n",
    "    log_prob = 0.0 # total log prob for the sentence\n",
    "    sentence_length = len(sentence)\n",
    "    word_pair_probs = []  \n",
    "\n",
    "    # Code for equation 1 \n",
    "    for index, center_word in enumerate(sentence):\n",
    "        if center_word not in model.wv:\n",
    "            if debug:\n",
    "                print(f\"Center word '{center_word}' not in vocabulary.\")\n",
    "            continue\n",
    "        center_vector = model.wv[center_word]\n",
    "\n",
    "        start = max(0, index - window)\n",
    "        end = min(sentence_length, index + window + 1)\n",
    "\n",
    "        for j in range(start, end):\n",
    "            if j == index:\n",
    "                continue\n",
    "            context_word = sentence[j]\n",
    "            if context_word not in model.wv:\n",
    "                if debug:\n",
    "                    print(f\"Context word '{context_word}' not in vocabulary.\")\n",
    "                continue\n",
    "            context_vector = model.wv[context_word]\n",
    "\n",
    "            dot_product = np.dot(center_vector, context_vector)\n",
    "            prob = sigmoid(dot_product)\n",
    "\n",
    "            word_pair_probs.append((center_word, context_word, prob))\n",
    "\n",
    "            log_prob += np.log(prob + 1e-10)\n",
    "\n",
    "    if debug:\n",
    "        print(\"\\n--- Word Pair Probabilities ---\")\n",
    "        for center, context, prob in word_pair_probs:\n",
    "            print(f\"p({context} | {center}) = {prob:.6f}\")\n",
    "\n",
    "    return log_prob\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Score an entire document (S sentences) under all models (Equation 2)\n",
    "# input (sentencces:  a list of sentences ,models: the dictionary of models, window: the window size for score sentences)\n",
    "# outpur: a sentences x categories (failed , succesful ....) with eahc sentence score according to score_sentence\n",
    "\n",
    "def score_document(sentences, models, window=5):\n",
    "    \"\"\"\n",
    "    Compute the score x category matrix of sentence scores for a document.\n",
    "    \n",
    "    sentences: list of sentences, each sentence is a list of words\n",
    "    models: dict of {category: Word2Vec model}\n",
    "    \"\"\"\n",
    "    S = len(sentences)\n",
    "    C = len(models)\n",
    "    \n",
    "    sentence_scores = np.zeros((S, C))\n",
    "    \n",
    "    for s_idx, sentence in enumerate(sentences):\n",
    "        for c_idx, (category, model) in enumerate(models.items()):\n",
    "            sentence_scores[s_idx, c_idx] = score_sentence(sentence, model, window)\n",
    "    \n",
    "    return sentence_scores\n",
    "\n",
    "\n",
    "\n",
    "# calculate document probabilities (Equation 5)\n",
    "\n",
    "# input: the sxc array\n",
    "# output: a 1x cateories array with the average score for all sentences in document \n",
    "def document_probabilities(sentence_scores):\n",
    "\n",
    "    return sentence_scores.mean(axis=0)\n",
    "\n",
    "\n",
    "\n",
    "# compute class probabilities ( Equation 3)\n",
    "\n",
    "# input:  the array from document_probabilities\n",
    "#ouput: normalized probabilities after bayes rule is applied #todo: change the priors to correspond to each class \n",
    "def class_probabilities(doc_probs):\n",
    "    \"\"\"\n",
    "    Compute class probabilities using Bayes rule.\n",
    "    Assuming uniform priors.\n",
    "    \"\"\"\n",
    "    priors = np.ones(len(doc_probs)) / len(doc_probs)\n",
    "    # bayes rule\n",
    "    probs = (doc_probs * priors) / np.sum(doc_probs * priors)\n",
    "    return probs\n",
    "\n",
    "\n",
    "\n",
    "# classify the document (Equation 6)\n",
    "# checks which of the numbers in the 1d array from document probabilities (the average across the classes ) is biggest and returns the index and array (for debuging) \n",
    " \n",
    "def classify_document(sentence_scores):\n",
    "    doc_probs = document_probabilities(sentence_scores)\n",
    "    predicted_class_idx = np.argmax(doc_probs)\n",
    "    return predicted_class_idx, doc_probs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluate the trained models \n",
    "\n",
    "def scoreSentences(listOfSentences, modelsList, predictedCategory):\n",
    "    success= 0\n",
    "    fail =0\n",
    "    \n",
    "    categories = list(modelsList.keys())\n",
    "    \n",
    "    for i, sentence_text in enumerate(listOfSentences, 1):\n",
    "        document = splitSentences(sentence_text)\n",
    "        sentence_scores = score_document(document, modelsList, window=5)\n",
    "        doc_probs = document_probabilities(sentence_scores)\n",
    "        probs = class_probabilities(doc_probs)\n",
    "        predicted_idx, doc_probs = classify_document(sentence_scores)\n",
    "        \n",
    "        print(f\" Predicting Sentence {document}\")\n",
    "        print(f\"\\nSentence {i}:\")\n",
    "        print(f\"Predicted class: {categories[predicted_idx]}\")\n",
    "        print(f\"Document probabilities: {doc_probs}\")\n",
    "        print(f\"Class probabilities: {probs}\")\n",
    "        \n",
    "        predicted_class = categories[predicted_idx]\n",
    "        \n",
    "        if (predicted_class == predictedCategory or\n",
    "            (predicted_class in [\"FailedCommons\", \"FailedLords\"] and \n",
    "             predictedCategory in [\"FailedCommons\", \"FailedLords\"])):\n",
    "            success += 1\n",
    "        else:\n",
    "            fail += 1\n",
    "    total = fail + success\n",
    "    \n",
    "    if total > 0:\n",
    "        accuracy = success / total\n",
    "        print(f\"Correct Prediction: {accuracy}\")\n",
    "    else:\n",
    "        print(\"Error in predicition.\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load models and populate models dictionar\n",
    "\n",
    "model_18 = Word2Vec.load(r\"C:\\Users\\ander\\Downloads\\MLP\\FailedCommons_word2vec_workers=4, hs=1, sg=1, negative=0, min_count=10, vector_size =300,window = 7.model\")\n",
    "model_19 = Word2Vec.load(r\"C:\\Users\\ander\\Downloads\\MLP\\FailedLords_word2vec_workers=4, hs=1, sg=1, negative=0, min_count=10, vector_size =300,window = 7.model\")\n",
    "model_29 = Word2Vec.load(r\"C:\\Users\\ander\\Downloads\\MLP\\SuccessLords_word2vec_workers=4, hs=1, sg=1, negative=0, min_count=10, vector_size =300,window = 7.model\")\n",
    "model_28 = Word2Vec.load(r\"C:\\Users\\ander\\Downloads\\MLP\\Model_28_re_trained_vs_100\")\n",
    "\n",
    "\n",
    "\n",
    "houseDictionary = {'FailedCommons': [18], 'FailedLords': [19], \"SuccesCommons\": [28], \"SuccessLords\":[29]}\n",
    "\n",
    "models ={}\n",
    "\n",
    "for key, indices in houseDictionary.items():\n",
    "    for index in indices:\n",
    "        if index == 18:\n",
    "            models[key] = model_18\n",
    "        elif index == 19:\n",
    "            models[key] = model_19\n",
    "        elif index == 28:\n",
    "            models[key] = model_28\n",
    "        elif index == 29:\n",
    "            models[key] = model_29\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methods to compute the scores for the sentences per individual cell and add the whole array \n",
    "#as well as the code for the actual predicted score \n",
    "from tqdm import tqdm\n",
    "def scoreDocumentCSV(text, models):\n",
    "    filtered_sentences = splitSentences(text)\n",
    "    sentence_scores = score_document(filtered_sentences, models, window=7)\n",
    "    index, doc_probs = classify_document(sentence_scores)\n",
    "    \n",
    "    return index, doc_probs\n",
    "        \n",
    "\n",
    "def appendScoresToFile(df, models, path):\n",
    "    df['predicted_class'] = None\n",
    "    df['doc_probs'] = None\n",
    "    categories = list(models.keys())\n",
    "\n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Processing rows\"):\n",
    "        result_index, probs_array = scoreDocumentCSV(row['extracted_text'], models)\n",
    "        \n",
    "        df.at[index, 'predicted_class'] = categories[result_index]\n",
    "        df.at[index, 'doc_probs'] = probs_array\n",
    "    df.to_csv(path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Df size 402\n",
      "DF size inside append scores 402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|███████████████████████████████████████████████████████████████| 402/402 [47:32<00:00,  7.10s/it]\n"
     ]
    }
   ],
   "source": [
    "## Score the training\n",
    "remaining = [28]\n",
    "basePath = r\"C:\\Users\\ander\\Downloads\\MLP\\cleanedTexWithID\" \n",
    "\n",
    "for i in remaining:\n",
    "    filePath = f\"{i}\\\\Training\\\\training_text{i}.csv\"\n",
    "    path = os.path.join(basePath, filePath)\n",
    "    df = pd.read_csv(path, error_bad_lines=False)  \n",
    "    print(f\"Df size {len(df)}\")\n",
    "    appendScoresToFile(df, models, path)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Df 28size 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|█████████████████████████████████████████████████████████████████| 45/45 [09:10<00:00, 12.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy of SuccesCommons is 0.26666666666666666\n",
      "Df 29size 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|█████████████████████████████████████████████████████████████████| 22/22 [00:27<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy of SuccessLords is 0.8181818181818182\n",
      "Df 18size 108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|███████████████████████████████████████████████████████████████| 108/108 [01:36<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy of FailedCommons is 0.6111111111111112\n",
      "Df 19size 57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|█████████████████████████████████████████████████████████████████| 57/57 [01:17<00:00,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy of FailedLords is 0.6666666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#### Score the validation and evaluate\n",
    "remaining = [28, 29, 18, 19]\n",
    "accessKeys = { 18: 'FailedCommons', 19: 'FailedLords', 28: \"SuccesCommons\", 29: \"SuccessLords\"}\n",
    "basePath = r\"C:\\Users\\ander\\Downloads\\MLP\\cleanedTexWithID\" \n",
    "\n",
    "\n",
    "models \n",
    "\n",
    "for i in remaining:\n",
    "    filePath = f\"{i}\\\\Validation\\\\validation_text{i}.csv\"\n",
    "    path = os.path.join(basePath, filePath)\n",
    "    df = pd.read_csv(path, error_bad_lines=False) \n",
    "    print(f\"Df {i}size {len(df)}\")\n",
    "    appendScoresToFile(df, models, path)\n",
    "    dfEvaluate = pd.read_csv(path, error_bad_lines=False) \n",
    "    \n",
    "    dfEvaluate['overall_accuracy'] = None\n",
    "    correct_bills = 0\n",
    "    for index, row in df.iterrows():\n",
    "        if row['predicted_class'] == accessKeys[i]:\n",
    "            correct_bills += 1\n",
    "    total_bills = len(dfEvaluate)\n",
    "    accuracy = correct_bills / total_bills\n",
    "    \n",
    "    print(F\" Accuracy of {accessKeys[i]} is {accuracy}\")\n",
    "    \n",
    "## For success commons , count success lords and success commons as predicted class and accuracy 0.8, if not 0.26   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FailedCommons\n",
      "FailedLords\n",
      "SuccesCommons\n",
      "SuccessLords\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
